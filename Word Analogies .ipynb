{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embedding Vectors to Find the Word Analogies\n",
    "\n",
    "* This method is based on using pre-trained word embedding vectors.\n",
    "\n",
    "In this method,for each word in a dictionary, a vector representation is produced - \"word embeddings\".\n",
    "\n",
    "This is a basic task in natural language processing. It is used in GRE and SAT exams in the US to help gauge human English ability!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained word vectors\n",
    "\n",
    "The file `pretrained_word2vec_vectors.txt` contains 100-dimensional word vectors for $N$ words. It is pretrained using the gensim package and the word2vec model.\n",
    "The first statement of the model has the number of words and dimensions for wach word.\n",
    "\n",
    "Representing the model in the form of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15174 word vectors.\n"
     ]
    }
   ],
   "source": [
    "filename = 'C:\\\\Users\\\\pooji\\\\Documents\\\\Notebooks\\\\95-865 Spring 2018 Mini-3 - Final Exam\\\\pretrained_word2vec_vectors.txt'\n",
    "import numpy as np\n",
    "embeddings_index= {}\n",
    "with open(filename) as f:\n",
    "    # each row represents a word vector\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # the first part is word\n",
    "        word = values[0]\n",
    "        # the rest of the values form the embedding vector\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the entry of the dictionary that has the information about the number of words and the dimensions of each vector (first line in the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = list(embeddings_index.keys())\n",
    "k[0]\n",
    "embeddings_index.pop('15173')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the word vectors into a numpy array `w2v` with 100 columns and $N$ rows.\n",
    "\n",
    "Construct a list `w2v_words` of $N$ words ordered according to the row index of their vector in `w2v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15173, 100)\n"
     ]
    }
   ],
   "source": [
    "w2v_words = [i for i,k in enumerate(embeddings_index.keys())]\n",
    "l = list(embeddings_index.values())\n",
    "w2v = np.array(l)\n",
    "print(w2v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize each vector by its L2 norm . After normalization, each row of `w2v` will have an L2 norm of 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = np.array([np.linalg.norm(v) for v in w2v])\n",
    "for i in range((w2v.shape[0])):\n",
    "    w2v[i] = w2v[i]/norm[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05401   ,  0.02648962, -0.13368909, -0.04295449, -0.11561567,\n",
       "        0.06392961,  0.18777601, -0.03032635, -0.03834768,  0.05719716,\n",
       "        0.04116647,  0.11574388,  0.05172624, -0.14440101,  0.04050237,\n",
       "        0.0799432 , -0.1046636 , -0.20766737,  0.01127687, -0.17266241,\n",
       "       -0.00200854, -0.07678851,  0.22929633,  0.07413639, -0.01899905,\n",
       "        0.02255801,  0.00419143, -0.11205757,  0.013129  ,  0.10333455,\n",
       "       -0.20436054,  0.03783913,  0.09512347, -0.07432272,  0.07157487,\n",
       "        0.17787179,  0.02194348,  0.16391374,  0.21379983, -0.04498867,\n",
       "        0.05303564, -0.12802587, -0.0914081 , -0.0825167 ,  0.24286807,\n",
       "       -0.07098598, -0.20567337, -0.09574313,  0.0079521 ,  0.08500044,\n",
       "        0.09422348, -0.04991087,  0.09977558, -0.14894287,  0.0044581 ,\n",
       "        0.14001128,  0.14962064,  0.02814773,  0.06051338, -0.14996679,\n",
       "       -0.077668  , -0.02580158,  0.07543638, -0.01375122, -0.09847303,\n",
       "        0.06246294,  0.02284519, -0.12523273, -0.10002857,  0.15161209,\n",
       "        0.07391845,  0.00197179, -0.01951614, -0.01641274,  0.04320663,\n",
       "        0.143307  ,  0.07501673,  0.05729118, -0.11867207,  0.06545353,\n",
       "       -0.12846945,  0.00513758,  0.05684075, -0.01159567,  0.0921175 ,\n",
       "        0.00126324,  0.01032816,  0.07293383,  0.05304846,  0.17024447,\n",
       "        0.00405725, -0.00612219, -0.00050256, -0.08963632, -0.04202629,\n",
       "       -0.20475797, -0.0649655 ,  0.04878096, -0.01128883,  0.06957146], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing whether your normalization is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.array([np.linalg.norm(v) for v in w2v]),\n",
    "                   np.ones(w2v.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting word analogies for new set using the pretrained model\n",
    "\n",
    "The `questions-words.txt` file is the test set that we are trying to predict the word analogies and test the accuracy. For instance,\n",
    "\n",
    "```\n",
    "Athens Greece Baghdad Iraq\n",
    "```\n",
    "\n",
    "This is - \"Athens is to Greece, as Baghdad is to ?\".\n",
    "\n",
    "The task is to predict the last word in each line. In this scenario, I am trying to assess how well our model can predict the word using pretrained model. Since, I am aware of the last word in each line, I will determine the number of successes and failures. \n",
    "\n",
    "Let `a b c d` be the four words in an analogy task. Let `v` be the word embedding, so `v[w]` is the vector for word `w`. \n",
    "\n",
    "Compute `pred = v[c] + (v[b] - v[a])`. This will be word vector which has the closest representation to c.\n",
    "\n",
    "We assess the closeness using the cosine distance of this computed vector with every vector in the model, and find the word `y` with the second highest cosine distance value. This is because the closes word to `pred` will most likely be `c`; hence, we use the second-closest instead. \n",
    "\n",
    "If `y` is equal to `d`, count the task as a success. If not, count it as a failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suc = 0 ##Number of succ\n",
    "fail = 0 ## Number of failures\n",
    "filename2 = 'C:\\\\Users\\\\pooji\\\\Documents\\\\Notebooks\\\\95-865 Spring 2018 Mini-3 - Final Exam\\\\questions-words.txt'\n",
    "with open(filename2) as f:\n",
    "    for line in f:\n",
    "        words =line.split()\n",
    "## Index in the keys list corresponds to the index in the w2v vector\n",
    "        word1_index = list(embeddings_index.keys()).index(words[1])\n",
    "        word2_index = list(embeddings_index.keys()).index(words[2])\n",
    "        word0_index = list(embeddings_index.keys()).index(words[0])\n",
    "        \n",
    "        pred = w2v[word2_index] + (w2v[word1_index] - w2v[word0_index])\n",
    "        cos = {}\n",
    "## cos is the cosine distance dictionary that has each word and its distance from the pred.\n",
    "        for i in range(w2v.shape[0]):\n",
    "            cos_sim = np.dot(pred,w2v[i])\n",
    "            cos[i] = cos_sim\n",
    "## to find the second highest cosine distance, sort it in descending order and find the element in the 1st position\n",
    "        k = sorted(list(cos.values()), reverse = True)[1]\n",
    "        ind = list(cos.values()).index(k)\n",
    "        pred_word = list(embeddings_index.keys())[ind]\n",
    "## to find the accuracy of the model, comparing predicted value with the test set\n",
    "        if pred_word == words[3]:\n",
    "            suc = suc + 1\n",
    "        else:\n",
    "            fail = fail + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'thinks', 'talk', 'talks']\n",
      "talk\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print(pred_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "5894\n"
     ]
    }
   ],
   "source": [
    "print(suc)\n",
    "print(fail)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
